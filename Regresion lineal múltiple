# Importamos las herramientas necesarias para el código.
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy.stats import t

''' Cargamos el dataset con todas las observaciones del vino. '''
vinos = pd.read_csv("winequality-white.csv", sep=";")

# Definimos las variables predictoras (X) y la variable respuesta (Y).
X = vinos[["fixed acidity", "volatile acidity", "citric acid", "residual sugar", "density", "pH", "alcohol"]].values
Y = vinos["quality"].values.reshape(-1, 1)

''' Antes de aplicar el método de descenso del gradiente, estandarizamos las variables predictoras para que todas tengan una escala comparable y así las que presentan valores grandes no dominen el proceso de ajuste.'''
scaler = StandardScaler()
X_std = scaler.fit_transform(X)

''' Agregamos una columna de unos al inicio de la matriz X para representar el término independiente (β₀) en el modelo.'''
n, p = X_std.shape
X_design = np.hstack((np.ones((n, 1)), X_std))

''' Inicializamos los coeficientes β en cero. 
Este vector se irá actualizando iterativamente hasta converger 
al mínimo del error cuadrático medio. '''
beta = np.zeros((p + 1, 1))

''' Definimos los híperparámetros del método: alpha representa la tasa de aprendizaje (qué tan grandes son los pasos de actualización) y epochs indica la cantidad de iteraciones. '''
alpha = 0.01   # tasa de aprendizaje
epochs = 10000 # iteraciones

''' Implementamos el bucle del método de descenso del gradiente. 
En cada iteración, calculamos las predicciones (y_hat), el error (diferencia con Y), el gradiente del error respecto a los coeficientes, 
y actualizamos β restando una fracción del gradiente. '''
for i in range(epochs):
    y_hat = X_design @ beta
    error = y_hat - Y
    grad = (2/n) * (X_design.T @ error)
    beta -= alpha * grad
''' Obtenemos los valores ajustados (ŷ) y los residuos (diferencia entre el valor real y el estimado). '''
y_hat = X_design @ beta
resid = Y - y_hat

''' Calculamos la varianza residual. '''
sigma2_hat = (resid.T @ resid) / (n - p - 1)

''' Determinamos el coeficiente de determinación R² y su versión ajustada (R²_adj), indicadores del grado de ajuste del modelo a los datos. '''
SST = np.sum((Y - Y.mean())**2)
SSE = np.sum(resid**2)
R2 = 1 - SSE / SST
R2_adj = 1 - (1 - R2) * (n - 1) / (n - p - 1)

''' Calculamos la matriz de covarianzas de los coeficientes.'''
cov_beta = sigma2_hat[0, 0] * np.linalg.inv(X_design.T @ X_design)
se_beta = np.sqrt(np.diag(cov_beta))

''' Calculamos los valores t y los p-valores correspondientes 
para evaluar la significancia estadística de cada coeficiente β̂. '''
t_values = beta.flatten() / se_beta

p_values = 2 * (1 - t.cdf(np.abs(t_values), df=n - p - 1))

''' Reunimos los principales resultados del modelo en un diccionario para organizarlos. '''
resultados = {
    "β̂": beta.flatten(),
    "σ̂²": sigma2_hat[0, 0],
    "R²": R2,
    "R² ajustado": R2_adj,
    "ŷ": y_hat.flatten(),
    "residuos": resid.flatten(),
}

''' Creamos una tabla resumen con los coeficientes estimados, 
sus errores estándar, valores t y p-valores.'''
variables = ["Intercepto"] + ["fixed acidity", "volatile acidity", "citric acid", "residual sugar", "density", "pH", "alcohol"]
tabla = pd.DataFrame({
    "Variable": variables,
    "β̂": beta.flatten(),
    "Error estándar": se_beta,
    "t": t_values,
    "p-valor": p_values
  })

''' Por último, imprimimos los resultados generales y los coeficientes del modelo. '''
print("\n Resultados Generales")
for k, v in resultados.items():
    if isinstance(v, float) or isinstance(v, np.float64):
        print(f"{k}: {v:.4f}")
    elif isinstance(v, np.ndarray):
        print(f"{k}: array con {len(v)} valores")
print("\n Coeficientes del Modelo")
print(tabla.round(4))
